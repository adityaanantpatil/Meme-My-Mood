ğŸ­ Emoji Reactor
Real-Time Facial Expression & Pose-Based Emoji Generator
<p align="center"> <img src="https://img.shields.io/badge/Python-3.8â€“3.10-3776AB?logo=python&logoColor=white" /> <img src="https://img.shields.io/badge/OpenCV-Enabled-5C3EE8?logo=opencv&logoColor=white" /> <img src="https://img.shields.io/badge/MediaPipe-FaceMesh%20%7C%20Pose-FF6F00?logo=google" /> <img src="https://img.shields.io/badge/Status-Stable-brightgreen?style=flat" /> </p> <p align="center"> <b>Live camera feed â†’ Detect Your Expression/Hands â†’ Show Matching Emoji</b><br> Fast â€¢ Stable â€¢ Anti-Flicker â€¢ Real-Time </p>
ğŸŒŸ Features



ğŸ¯ Real-Time Emotion Detection
Smile ğŸ˜€
Surprised ğŸ˜®
Neutral ğŸ˜


ğŸ™Œ Pose Detection
Hands-up detection using full body Pose landmarks

ğŸ§  Stabilized Output
10-frame smoothing buffer
Normalized landmark distances


ğŸªŸ Dual Window UI
Camera Feed with live state
Emoji Output



ğŸ“ Fully Offline â€” No Internet required

Raise both hands â†’ ğŸ™Œ
Smile â†’ ğŸ˜€
Say "wow" with wide mouth â†’ ğŸ˜®
Neutral face â†’ ğŸ˜

ğŸ§© Project Structure
emoji-reactor/
â”‚â”€â”€ emoji_reactor.py
â”‚â”€â”€ images/
â”‚     â”œâ”€â”€ smile.jpg
â”‚     â”œâ”€â”€ plain.jpg
â”‚     â”œâ”€â”€ air.jpg
â”‚     â”œâ”€â”€ surprised.jpg
â”‚â”€â”€ README.md

ğŸ”§ Installation
1ï¸âƒ£ Install dependencies
pip install opencv-python mediapipe numpy


âš ï¸ MediaPipe requires Python 3.10 or lower â€” 3.11/3.12+ may cause import errors.


2ï¸âƒ£ Run the program
python emoji_reactor.py

ğŸ–¼ï¸ Required Emoji Files
Inside the /images folder, include:
smile.jpg â†’ for smiling
plain.jpg â†’ neutral
air.jpg â†’ hands up
surprised.jpg â†’ surprised face


All are automatically resized to fit the emoji window.

ğŸ§  How It Works (Technical Breakdown)
Pose Module
Wrist Y-coordinate < Shoulder Y-coordinate
â†’ triggers HANDS UP
Face Mesh Module


Extracts:
Eye corners
Mouth corners
Upper & lower inner lips


Computes:
eye_distance â†’ normalization
mouth_open_distance
Rolling average (10-frame anti-flicker)



Decision Logic

State	Condition
ğŸ™Œ HANDS_UP	Wrist above shoulder
ğŸ˜€ SMILING	mouth_open > 0.11
ğŸ˜® SURPRISED	mouth_open > 0.22
ğŸ˜ STRAIGHT_FACE	everything else
ğŸ–¥ï¸ Controls
Key	Action

q	Quit the program



ğŸ§ª Upcoming: Custom Facial Expression Model (WIP)

A new deep-learningâ€“based Facial Expression Recognition (FER) model is currently under development and will soon replace/augment the MediaPipe mouth-distance logic.

ğŸš€ What This Model Will Do

Detect 7+ emotions with higher accuracy
ğŸ˜€ Happy
ğŸ˜ Neutral
ğŸ˜® Surprise
ğŸ˜¡ Angry
ğŸ˜¢ Sad
ğŸ˜¤ Disgust
ğŸ˜  Contempt (optional)

Provide stable predictions using softmax smoothing.
Reduce false detections caused by lighting, angle, and head pose.
Fully offline â€” no cloud API needed.



ğŸ§± Architecture (Planned)
Lightweight CNN or MobileNetV3-based classifier
Trained on FER-2013 / RAF-DB / custom dataset
Uses cropped 48Ã—48 or 112Ã—112 grayscale/RGB facial images
Optimized for real-time inference on CPU


ğŸ”„ Integration Plan

The pipeline will soon look like:
Camera â†’ Face Detection â†’ FER Model â†’ Expression Label â†’ Emoji Output

This will replace the current:
Camera â†’ Face Mesh â†’ Landmark Distances â†’ Emoji Output


The system will auto-switch:

Engine	Status
MediaPipe landmark-based expressions	Active (Current)
Custom FER deep learning model	Coming Soon
ğŸ› ï¸ Experimental Mode (Optional)

A toggle USE_CUSTOM_MODEL = True will allow developers to test the new model once the .h5 or .pt file is added to:
/model/emotion_model.pt


Activation plan inside emoji_reactor.py:

# TODO: Enable when model is ready
USE_CUSTOM_MODEL = False  

if USE_CUSTOM_MODEL:
    # Predict using the custom FER model
    expression = fer_model.predict(face_crop)
else:
    # Fallback to MediaPipe expression logic
    expression = mediapipe_expression_logic()
